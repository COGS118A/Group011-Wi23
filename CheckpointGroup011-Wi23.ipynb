{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Joshua Howon Kim \n",
    "- Yewon Hong\n",
    "- Seonghun Oh "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "As social media has become used from almost every person in the world, it has been easier for people to receive and share news. However, this easy access of news has also allowed it to be much easier to spread fake news. There are many platforms that purposely spread fake and biased news for their own benefit. It is very difficult for individuals to tell whether news is fake or real. Thus, this intentional spread of fake news is able to bring up many social conflicts, leading to disasters in the society. Therefore, it is very important for people to be able to correctly identify if the news they are reading is real or fake. The goal of our project is to solve this problem by using machine learning classifiers and correctly detecting if a news is real or fake. The data we are using is scraped from snopes.com which is a website that fact checks and analyzes information from the news. The variables in the dataset include: the title, comments by the public on the fact, claims to support the comment, label of the news(True, False, Miscaptioned), summary of the content, information that are true, false and unknown. We will be using these data to understand how each variable affects on the validity of the news content. In order to validate if the news is fake or not, we will be using multivariable logistic regression.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "As SNS spreads rapidly, fake news disguised as the media remains a major social problem. Fake news has become a particular public issue for the following reason:As online public opinion spheres are activated, people have diverse and easy access to platforms such as YouTube, and SNS. Unverified issues are produced and spread easily in that they can be freely spoken and shared information and opinions, making it easy for anyone to make news that may be extreme or wrongful, and spread them. Since 2017, academic research and discussion have gotten in earnest in the United States, where fake news remarked by Trump had been shifted to fake news.[<sup>1</sup>](#fn1)According to previous discussions of fake news, fake news has been broadly defined as A) for political and economic gain, B) deliberately distorting and fabricating, and C) false information disguised as media coverage. [<sup>2</sup>](#fn2)With the introduction of artificial intelligence and various technology to distinguish fake news and unidentified information by SNS, media companies, also with the emergence of several fact-checking media show that media literacy has become a must for modern people. [<sup>3</sup>](#fn3) Machine Learning(ML) algorithm can be the solution. At first, the use of ML algorithm can detect fake news easily and automatically. In addition, the dataset can easily be collected to train calssifiers, especially supervised calssifiers. Since these classifiers depend on labelled datasets, by training these classifiers, we can identify fake news more accurately than from a human perspective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Fake news damages certain people's reputation, violates someone’s privacy and personality rights and causes false prejudices against specific groups. Furthermore, it reduces the credibility of established media and other opinions and causes distortions in the process of forming political opinions. This causes social confusion and division, which harms civil society and its members and society as a whole. This is because it can cover facts and truths and spread false or distorted information to hinder the formation of sound public opinion. Naturally, such crackdowns and regulations on fake news are necessary. However, the biggest problem with fake news that we faced is that the current concept of fake news can be widespread and ambiguous. This is why the priority should be to find out what fake news is if legislation is to save damage to fake news. In a world of millions of news, they rely on manual human detection, so their scope is so limited that if the fake news is posted and deleted every minute, they cannot be manually responsible or executed. It can be a solution through the development of a system that provides reliable, automated exponential scores, namely trained machine learning.  solution through the development of a system that provides reliable, automated exponential scores, namely trained machine learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "UPDATED FROM PROPOSAL!\n",
    "\n",
    "You should have obtained and cleaned (if necessary) data you will use for this project.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "Benchmark model: \n",
    "\n",
    "\n",
    "Solution: multivariable logistic regression\n",
    "\n",
    "Library used: numpy, pandas, scikit-learn\n",
    "\n",
    "Since our problem is a multiclass classification of evaluating if the news is True, Mostly-true, Half-true, Barely-true, FALSE, Pants-fire (6 labels), we will be using multi-class logistic regression. In order to process the test into input data, we will be using vectorization. Then we will be adding this vectorized input to variables from multiple columns and evaluate whether the news is true or false. Thus, we will be using multivariable logistic regression. First of all, we will be dividing the dataset into training/validation set and test set. Next, because vectorizing the text into word units creates too many variables, we will be using normalization. Because we will be needing strong normalization, we believe that using L1 normalization will be effective. The search plane itself is a vectorized text, therefore it is possible for it to become very high dimension which may result in being stuck in the local minima. In order to prevent this problem, we will be using momentum in our training. Also, in order to quickly do weight updates in training, we will be using mini-batch. In the validation step, we will be using K-fold cross validation to retreive the most accurate model.\n",
    "<Text Preprocessing>\n",
    "After converting articles into token lists through CountVectorizer, TfidTransformer, and TfidVectorizer, vectorize them based on the count and frequency of appearance of tokens. In addition, TfidVectorier creates a BOW encoding vector that weights words in a TF-IDF, enabling document preprocessing. Using the TF-IDF method, you can manually adjust the parameter C to use an estimate that is a LogisticRegressionCV. Specifying cv=5, using kfold cross-validation for hyperparameter tuning. The criterion for model measurement is the accuracy of classification, and by setting it to n_jobs=-1, all CPU cores are dedicated. We’ll evaluate the performance by maximizing the number of iterations of the optimization algorithm.\n",
    "\n",
    "<Modeling>\n",
    "As a classification model, machine learning algorithms can be used : \n",
    "a) Logistic Registration, b) SVM, c) Random Forest, and boosted trees (AdaBoost)\n",
    "\n",
    "First, modeling using the logistic regression algorithm. For modeling, separate train, validation sets and test set. Using the same dataset and the same text method and train_test_split, we can create logistic regression models that classify news as real or fake news. After organizing and preprocessing text data, performing feature extraction, and building and distributing a logistic regression classifier using the scikit-learn, evaluate the accuracy of the model.\n",
    "\n",
    "Second, Using SVM(Support Vector Machines) to build classification prediction models and compare prediction accuracy on datasets for verification.\n",
    "Third, tree-based random forest. Using Adaboost to fit the shallow decision tree on the train set, and additional classifiers can be placed on the same data to correct errors and can be weighted. \n",
    "\n",
    "Also, k-fold cross-validation, which is known to be useful because the amount of data is not enough to experiment with a larger amount of data on a smaller dataset to increase the statistical reliability of methodological performance measurements.\n",
    "\n",
    "<Evaluation>\n",
    "Confusion matrix : f1_score, report\n",
    "Precision, recall \n",
    "\n",
    "< what we see moving forward and limitations>\n",
    "Using RNN, CNN, and LSTM, which are algorithms of deep learning could have analyzed with better performance. However, we need to review whether there are enough samples for deep learning, and we can later create a model using appropriate algorithms by devising how to classify new articles that we have never seen before.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The evaluation metrics we will be using will be True positive rate (recall) and Accuracy. For recall, we will be using (correctly evaluating that news is false) / (correctly evaluating that news is false + evaluating that news is true when it is actually false). This recall rate is important for our problem because it tells us the rate of us being able to correctly evaluate that the news is false. For Accuracy, we will be using (correctly evaluating that news is false + evaluating that news is false when it is actually true) / (correctly evaluating that news is false + correctly evaluating that news is true + evaluating that news is true when it is actually false + evaluating that news is false when it is actually true). Accuracy is important for our problem because it tells us if our model was able to accurately make correct predictions in evaluating that the news is false. We will then plot an ROC curve to show the performance of the classification model and then find the AUC to provide an aggregate measure of performance across all possible classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, there may be some potential ethical concerns because we are retrieving our data from previously published news articles. Because we are basically evaluating whether a person’s article is true or false, in some way, we are criticizing the person’s work. Therefore, by providing data that a person’s work is wrong, it may harm the person’s career. We also think that this may also bring up privacy issues because we are using someone else’s work and evaluating it without the publisher’s permission. However, we believe that this will not be that much of a problem because news itself will always be biased in some sort of way and it is the reader’s responsibility to take in only correct information. Therefore, we believe that our project serves a good purpose because it helps the reader’s to make correct decisions in trusting the correct news article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contact over group chat\n",
    "* Respond as quickly as possible within 24 hours\n",
    "* Meetings will happen at least once a week and all members have to attend\n",
    "* All members are expected and responsible for finishing their divided work\n",
    "* All members should review other's work and provide feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Agenda  | Finish by next meeting |\n",
    "|---|---|---|---|\n",
    "| 2/22  |  4 PM |  Finalize proposal and turn it in  | Think about how we are going to make use of our datasets | \n",
    "| 2/27  | 12 PM | Discuss and brainstorm EDA  | Make progress in EDA and add to preliminary results section in checkpoint| \n",
    "| 3/3  | 12 PM | Look at Proposal Grade & Feedback and check what we have to improve on  | Revise the sections in the proposal that the TAs and Professor gave feedback on |\n",
    "| 3/5  | 12 PM  | Share EDA progress | Finalize revising sections that were in the feedback and complete preliminary results section |\n",
    "| 3/8  | 4 PM  | Final revision of checkpoint and turn it in | Continue working on project |\n",
    "| 3/19  | Before 11:59 PM  | Final revision of entire project and be ready to turn in | Turn in Final Project  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"holannote\"></a>1.[^](#holan): holan, Angie D. The Media's Definition of Fake News vs. Donald Trump's. https://scholarship.law.unc.edu/cgi/viewcontent.cgi?article=1244&context=falr.<br>\n",
    "<a name=\"westnote\"></a>2.[^](#west): west, Darrell M. “How to Combat Fake News and Disinformation.” Brookings, Brookings, 9 Mar. 2022, https://www.brookings.edu/research/how-to-combat-fake-news-and-disinformation/.<br>\n",
    "<a name=\"arxivnote\"></a>3.[^](#arxive): Detecting Fake News using Machine Learning: A Systematic Literature Review https://arxiv.org/pdf/2102.04458.pdf<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
