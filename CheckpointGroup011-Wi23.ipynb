{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Joshua Howon Kim \n",
    "- Yewon Hong\n",
    "- Seonghun Oh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "As social media has become used from almost every person in the world, it has been easier for people to receive and share news. However, this easy access of news has also allowed it to be much easier to spread fake news. There are many platforms that purposely spread fake and biased news for their own benefit. It is very difficult for individuals to tell whether news is fake or real. Thus, this intentional spread of fake news is able to bring up many social conflicts, leading to disasters in the society. Therefore, it is very important for people to be able to correctly identify if the news they are reading is real or fake. The goal of our project is to solve this problem by using machine learning classifiers and correctly detecting if a news is real or fake. The data we are using is scraped from snopes.com which is a website that fact checks and analyzes information from the news. The variables in the dataset include: the title, comments by the public on the fact, claims to support the comment, label of the news (True, False, Miscaptioned), summary of the content, information that are true, false and unknown. We will be using these data to understand how each variable affects on the validity of the news content. In order to validate if the news is fake or not, we will be using multivariable logistic regression and SVM.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "As SNS spreads rapidly, fake news disguised as the media remains a major social problem. Fake news has become a particular public issue for the following reason:As online public opinion spheres are activated, people have diverse and easy access to platforms such as YouTube, and SNS. Unverified issues are produced and spread easily in that they can be freely spoken and shared information and opinions, making it easy for anyone to make news that may be extreme or wrongful, and spread them. Since 2017, academic research and discussion have gotten in earnest in the United States, where fake news remarked by Trump had been shifted to fake news.[<sup>1</sup>](#fn1)According to previous discussions of fake news, fake news has been broadly defined as A) for political and economic gain, B) deliberately distorting and fabricating, and C) false information disguised as media coverage. [<sup>2</sup>](#fn2)With the introduction of artificial intelligence and various technology to distinguish fake news and unidentified information by SNS, media companies, also with the emergence of several fact-checking media show that media literacy has become a must for modern people. [<sup>3</sup>](#fn3) Machine Learning(ML) algorithm can be the solution. At first, the use of ML algorithm can detect fake news easily and automatically. In addition, the dataset can easily be collected to train calssifiers, especially supervised calssifiers. Since these classifiers depend on labelled datasets, by training these classifiers, we can identify fake news more accurately than from a human perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Fake news damages certain people's reputation, violates someone’s privacy and personality rights and causes false prejudices against specific groups. Furthermore, it reduces the credibility of established media and other opinions and causes distortions in the process of forming political opinions. This causes social confusion and division, which harms civil society and its members and society as a whole. This is because it can cover facts and truths and spread false or distorted information to hinder the formation of sound public opinion. Naturally, such crackdowns and regulations on fake news are necessary. However, the biggest problem with fake news that we faced is that the current concept of fake news can be widespread and ambiguous. This is why the priority should be to find out what fake news is if legislation is to save damage to fake news. In a world of millions of news, they rely on manual human detection, so their scope is so limited that if the fake news is posted and deleted every minute, they cannot be manually responsible or executed. It can be a solution through the development of a system that provides reliable, automated exponential scores, namely trained machine learning.  solution through the development of a system that provides reliable, automated exponential scores, namely trained machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Dataset Link: ",
    "https://github.com/COGS118A/Group011-Wi23/tree/hun/liar_dataset\n",
    "\n",
    "8 variables, 4625 unique values\n",
    "\n",
    "Snopes Fact-news Data | Kaggle\n",
    "\n",
    "Question: News/Fact's title\n",
    "\n",
    "Comment: A short comment by the public on the fact\n",
    "\n",
    "Claim: Claim to support the comment\n",
    "\n",
    "Rate: label of the news(True, False, Miscaptioned etc.)\n",
    "\n",
    "Origin: Content of the page\n",
    "\n",
    "summary: Short Version of origin feature\n",
    "\n",
    "\n",
    "Label dependent features:\n",
    "\n",
    "What's True: True about the fact\n",
    "\n",
    "What's False: False about the fact\n",
    "\n",
    "What's Unknown: What's Unknown about the fact\n",
    "\n",
    "\n",
    "We propose a methodology that applies supervised machine learning algorithms to a label dependent features dataset to create a model that detects whether an article is true or fake based on Question, Comment, Claim, Rate, and summary features. \n",
    "\n",
    "\n",
    "\n",
    "Also, we found the other dataset deals with:\n",
    "\n",
    "Post number, article unique number, title, article title, URL, article link, summary, article summary, article publication date, article publication date, revision date, category, category of  fake news subject, Claim, Veracity (Evaluation result of verified rumor. True, can have values such as most true, most false, etc.), source of information (the source of information used to verify the fake news), It was confirmed that there is a Snopes dataset with a total of 16 features. We can obtain useful information from this dataset via Claim and Veracity.\n",
    "\n",
    "—----------------------------—----------------------------—----------------------------—-------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dataset from William Yang Wang, \"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection, to appear in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), short paper, Vancouver, BC, Canada, July 30-August 4, ACL.\n",
    "\n",
    "##### DATASET DESCRIPTION\n",
    "Column 1: the ID of the statement ([ID].json).\n",
    "\n",
    "Column 2: the label. (Label class contains: True, Mostly-true, Half-true, Barely-true, FALSE, Pants-fire)\n",
    "\n",
    "Column 3: the statement.\n",
    "\n",
    "Column 4: the subject(s).\n",
    "\n",
    "Column 5: the speaker.\n",
    "\n",
    "Column 6: the speaker's job title.\n",
    "\n",
    "Column 7: the state info.\n",
    "\n",
    "Column 8: the party affiliation.\n",
    "\n",
    "For Column 3 - 8 are all discrete variables and these all we barely used.\n",
    "\n",
    "Column 9-13: the total credit history count, including the current statement.\n",
    "\n",
    "9: barely true counts.\n",
    "\n",
    "10: false counts.\n",
    "\n",
    "11: half true counts.\n",
    "\n",
    "12: mostly true counts.\n",
    "\n",
    "13: pants on fire counts.\n",
    "\n",
    "Column 14: the context (venue / location of the speech or statement).\n",
    "\n",
    "\n",
    "We propose a methodology that applies supervised machine learning algorithms to a label dependent features dataset to create a model that detects whether an article is true or fake based on Statement, Subject, Speaker, Speaker's job title, State info and Party affiliation. Also, we found the other dataset deals with:barely true counts, false counts, half true counts, mostly true counts, pants on fire counts. If we have to use binary classifier, our column 9-13 will be labeled as\n",
    "\n",
    "True -- True\n",
    "\n",
    "Mostly-true -- True\n",
    "\n",
    "Half-true -- True\n",
    "\n",
    "Barely-true -- False\n",
    "\n",
    "False -- False\n",
    "\n",
    "Pants-fire -- False.\n",
    "\n",
    "\n",
    "With this, we can classify every statement as true/false. Without this, there are 5 labels on our dataset, the classification task goes to multi-class classification, which we have to use softmax regression (multi-class logistic regression) At first, we want to try softmax regression to see how true the statement is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "Benchmark model: \n",
    "\n",
    "Solution: multivariable logistic regression\n",
    "\n",
    "Library used: numpy, pandas, scikit-learn\n",
    "\n",
    "Since our problem is a multiclass classification of evaluating if the news is True, Mostly-true, Half-true, Barely-true, FALSE, Pants-fire (6 labels), we will be using multi-class logistic regression. In order to process the test into input data, we will be using vectorization. Then we will be adding this vectorized input to variables from multiple columns and evaluate whether the news is true or false. Thus, we will be using multivariable logistic regression. First of all, we will be dividing the dataset into training/validation set and test set. Next, because vectorizing the text into word units creates too many variables, we will be using normalization. Because we will be needing strong normalization, we believe that using L1 normalization will be effective. The search plane itself is a vectorized text, therefore it is possible for it to become very high dimension which may result in being stuck in the local minima. In order to prevent this problem, we will be using momentum in our training. Also, in order to quickly do weight updates in training, we will be using mini-batch. In the validation step, we will be using K-fold cross validation to retreive the most accurate model. \n",
    "\n",
    "###### Text Processing\n",
    "After converting articles into token lists through CountVectorizer, TfidTransformer, and TfidVectorizer, vectorize them based on the count and frequency of appearance of tokens. In addition, TfidVectorier creates a BOW encoding vector that weights words in a TF-IDF, enabling document preprocessing. Using the TF-IDF method, you can manually adjust the parameter C to use an estimate that is a LogisticRegressionCV. Specifying cv=5, using kfold cross-validation for hyperparameter tuning. The criterion for model measurement is the accuracy of classification, and by setting it to n_jobs=-1, all CPU cores are dedicated. We’ll evaluate the performance by maximizing the number of iterations of the optimization algorithm.\n",
    "\n",
    "###### Modeling\n",
    "As a classification model, machine learning algorithms can be used \n",
    "a) Logistic Registration, b) SVM, c) Random Forest, and boosted trees(AdaBoost)\n",
    "\n",
    "First, modeling using the logistic regression algorithm.\n",
    "For modeling, separate train, validation sets and test set.\n",
    "Using the same dataset and the same text method and train_test_split, we can create logistic regression models that classify news as real or fake news. After organizing and preprocessing text data, performing feature extraction, and building and distributing a logistic regression classifier using the scikit-learn, evaluate the accuracy of the model.\n",
    "Second, Using SVM(Support Vector Machines) to build classification prediction models and compare prediction accuracy on datasets for verification.\n",
    "Third, tree-based random forest. Using Adaboost to fit the shallow decision tree on the train set, and additional classifiers can be placed on the same data to correct errors and can be weighted. \n",
    "Also, k-fold cross-validation, which is known to be useful because the amount of data is not enough to experiment with a larger amount of data on a smaller dataset to increase the statistical reliability of methodological performance measurements.\n",
    "\n",
    "###### Evaluation\n",
    "Confusion matrix : f1_score, report / Precision, recall \n",
    "\n",
    "\n",
    "###### What we see moving forward and limitations\n",
    "Using RNN, CNN, and LSTM, which are algorithms of deep learning could have analyzed with better performance. However, we need to review whether there are enough samples for deep learning, and we can later create a model using appropriate algorithms by devising how to classify new articles that we have never seen before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The evaluation metrics we will be using will be True positive rate (recall) and Accuracy. For recall, we will be using (correctly evaluating that news is false) / (correctly evaluating that news is false + evaluating that news is true when it is actually false). This recall rate is important for our problem because it tells us the rate of us being able to correctly evaluate that the news is false. For Accuracy, we will be using (correctly evaluating that news is false + evaluating that news is false when it is actually true) / (correctly evaluating that news is false + correctly evaluating that news is true + evaluating that news is true when it is actually false + evaluating that news is false when it is actually true). Accuracy is important for our problem because it tells us if our model was able to accurately make correct predictions in evaluating that the news is false. We will then plot an ROC curve to show the performance of the classification model and then find the AUC to provide an aggregate measure of performance across all possible classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = './liar_dataset/test.tsv'\n",
    "train_filename = './liar_dataset/train.tsv'\n",
    "valid_filename = './liar_dataset/valid.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './liar_dataset/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c_/sch32ylx6b541jfnlv91jrrr0000gn/T/ipykernel_49293/2702683750.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ID.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'statement'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'speaker'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"speaker's job title\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'party affiliation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'barely true counts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'false counts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'half true counts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mostly true counts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pants on fore counts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'the context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data_origin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_data_origin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid_data_origin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './liar_dataset/train.tsv'"
     ]
    }
   ],
   "source": [
    "column_name = ['ID.json','label','statement','subject','speaker',\"speaker's job title\",'state','party affiliation','barely true counts','false counts','half true counts','mostly true counts','pants on fore counts','the context']\n",
    "train_data_origin = pd.read_csv(train_filename, sep=\"\\t\")\n",
    "test_data_origin = pd.read_csv(test_filename, sep=\"\\t\")\n",
    "valid_data_origin = pd.read_csv(valid_filename, sep=\"\\t\")\n",
    "\n",
    "train_data = train_data_origin.values.tolist()\n",
    "test_data = test_data_origin.values.tolist()\n",
    "valid_data = valid_data_origin.values.tolist()\n",
    "\n",
    "train_data = pd.DataFrame(train_data, columns=column_name)\n",
    "test_data = pd.DataFrame(test_data, columns=column_name)\n",
    "valid_data = pd.DataFrame(valid_data, columns=column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label and statement\n",
    "train_news = train_data.iloc[:,1:3]\n",
    "test_news = test_data.iloc[:,1:3]\n",
    "valid_news = test_data.iloc[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the unique value of label\n",
    "print(train_news['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace original label to binary label\n",
    "train_news = train_news.replace({'label' : { 'half-true' : 1, 'false' : 0, 'mostly-true' : 1 , 'true' : 1, 'barely-true' : 1, 'pants-fire' : 0}})\n",
    "test_news = test_news.replace({'label' : { 'half-true' : 1, 'false' : 0, 'mostly-true' : 1 , 'true' : 1, 'barely-true' : 1, 'pants-fire' : 0}})\n",
    "valid_news = valid_news.replace({'label' : { 'half-true' : 1, 'false' : 0, 'mostly-true' : 1 , 'true' : 1, 'barely-true' : 1, 'pants-fire' : 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_news['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribution(dataFile):\n",
    "    return sb.countplot(x='label', data=dataFile, palette='hls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by calling below we can see that training, test and valid data seems to be failry evenly distributed between the classes\n",
    "create_distribution(train_news)\n",
    "create_distribution(test_news)\n",
    "create_distribution(valid_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing label\n",
    "def data_qualityCheck():\n",
    "    print('train data')\n",
    "    print(train_news.isnull().sum())\n",
    "    train_news.info()\n",
    "    print('\\n')\n",
    "        \n",
    "    print('test data')\n",
    "    print(test_news.isnull().sum())\n",
    "    test_news.info()\n",
    "    print('\\n')\n",
    "\n",
    "    print('valid data')\n",
    "    print(valid_news.isnull().sum())\n",
    "    valid_news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qualityCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature select\n",
    "pip install -U gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countV = CountVectorizer()\n",
    "train_count = countV.fit_transform(train_news['statement'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_count.shape)\n",
    "# (# of statement, # of word(all word in every statement) in statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(countV.get_feature_names_out()[1000:1010])\n",
    "#print(countV.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install import_ipynb\n",
    "! pip install sklearn_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import Feature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn_evaluation\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train, test, validation set\n",
    "X_train = Feature.train_news['statement']\n",
    "y_train = Feature.train_news['label']\n",
    "\n",
    "X_test = Feature.test_news['statement']\n",
    "y_test = Feature.test_news['label']\n",
    "\n",
    "X_val = Feature.valid_news['statement']\n",
    "y_val = Feature.valid_news['label']\n",
    "\n",
    "# define whole dataset\n",
    "X = pd.concat([X_train, X_test, X_val])\n",
    "y = pd.concat([y_train, y_test, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classification function using model\n",
    "def classification(clf, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # using 5 fold cross validation\n",
    "    train_size,train_score,test_score = learning_curve(clf, X_train, y_train, cv=5)\n",
    "    sklearn_evaluation.plot.learning_curve(train_score, test_score, train_size)\n",
    "    \n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    clf_table = classification_report(y_test, y_pred, target_names=['false', 'true'])\n",
    "    \n",
    "    return acc, clf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using bag of word (CounterVectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanila logistic regression\n",
    "model_lgr_vanila = Pipeline([\n",
    "        ('feature_extractor',Feature.countV),\n",
    "        ('clf',LogisticRegression())\n",
    "        ])\n",
    "\n",
    "# set l2 penalty and C = 0.01 (need to regularize)\n",
    "model_lgr = Pipeline([\n",
    "        ('feature_extractor',Feature.countV),\n",
    "        ('lgr_clf',LogisticRegression(penalty='l2', C=0.01))\n",
    "        ])\n",
    "\n",
    "# vanila Linear SVM\n",
    "model_svm_vanila = Pipeline([\n",
    "        ('feature_extractor',Feature.countV),\n",
    "        ('svm_clf',svm.LinearSVC())\n",
    "        ])\n",
    "\n",
    "# vanila Linear SVM\n",
    "model_svm = Pipeline([\n",
    "        ('feature_extractor',Feature.countV),\n",
    "        ('svm_clf',svm.LinearSVC(loss = 'hinge', C = 0.1))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanila logistic regression\n",
    "acc, table = classification(model_lgr_vanila, X, y)\n",
    "print(\"accuracy : \",acc)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic reguression with l2 regularization and less C\n",
    "acc, table = classification(model_lgr, X, y)\n",
    "print(\"accuracy : \",acc)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanila linear svm\n",
    "acc, table = classification(model_svm_vanila, X, y)\n",
    "print(\"accuracy : \",acc)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear svm with hinge loss\n",
    "acc, table = classification(model_svm, X, y)\n",
    "print(\"accuracy : \",acc)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, there may be some potential ethical concerns because we are retrieving our data from previously published news articles. Because we are basically evaluating whether a person’s article is true or false, in some way, we are criticizing the person’s work. Therefore, by providing data that a person’s work is wrong, it may harm the person’s career. We also think that this may also bring up privacy issues because we are using someone else’s work and evaluating it without the publisher’s permission. However, we believe that this will not be that much of a problem because news itself will always be biased in some sort of way and it is the reader’s responsibility to take in only correct information. Therefore, we believe that our project serves a good purpose because it helps the reader’s to make correct decisions in trusting the correct news article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contact over group chat\n",
    "* Respond as quickly as possible within 24 hours\n",
    "* Meetings will happen at least once a week and all members have to attend\n",
    "* All members are expected and responsible for finishing their divided work\n",
    "* All members should review other's work and provide feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Agenda  | Finish by next meeting |\n",
    "|---|---|---|---|\n",
    "| 2/22  |  4 PM |  Finalize proposal and turn it in  | Think about how we are going to make use of our datasets | \n",
    "| 2/27  | 12 PM | Discuss and brainstorm EDA  | Make progress in EDA and add to preliminary results section in checkpoint| \n",
    "| 3/3  | 12 PM | Look at Proposal Grade & Feedback and check what we have to improve on  | Revise the sections in the proposal that the TAs and Professor gave feedback on |\n",
    "| 3/5  | 12 PM  | Share EDA progress | Finalize revising sections that were in the feedback and complete preliminary results section |\n",
    "| 3/8  | 4 PM  | Final revision of checkpoint and turn it in | Continue working on project |\n",
    "| 3/19  | Before 11:59 PM  | Final revision of entire project and be ready to turn in | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"holannote\"></a>1.[^](#holan): holan, Angie D. The Media's Definition of Fake News vs. Donald Trump's. https://scholarship.law.unc.edu/cgi/viewcontent.cgi?article=1244&context=falr.<br>\n",
    "<a name=\"westnote\"></a>2.[^](#west): west, Darrell M. “How to Combat Fake News and Disinformation.” Brookings, Brookings, 9 Mar. 2022, https://www.brookings.edu/research/how-to-combat-fake-news-and-disinformation/.<br>\n",
    "<a name=\"arxivnote\"></a>3.[^](#arxive): Detecting Fake News using Machine Learning: A Systematic Literature Review https://arxiv.org/pdf/2102.04458.pdf<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
