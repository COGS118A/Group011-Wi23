{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Final Project: Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Joshua Howon Kim\n",
    "- Yewon Hong\n",
    "- Seonghun Oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "As social media has become used from almost every person in the world, it has been easier for people to receive and share news. However, this easy access of news has also allowed it to be much easier to spread fake news. There are many platforms that purposely spread fake and biased news for their own benefit. It is very difficult for individuals to tell whether news is fake or real. Thus, this intentional spread of fake news is able to bring up many social conflicts, leading to disasters in the society. Therefore, it is very important for people to be able to correctly identify if the news they are reading is real or fake. The goal of our project is to solve this problem by using machine learning classifiers and correctly detecting if a news is real or fake. The data we are using is scraped from snopes.com which is a website that fact checks and analyzes information from the news. The variables in the dataset include: the title, comments by the public on the fact, claims to support the comment, label of the news (True, False, Miscaptioned), summary of the content, information that are true, false and unknown. We will be using these data to understand how each variable affects on the validity of the news content. In order to validate if the news is fake or not, we will be using multivariable logistic regression and SVM.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "As SNS spreads rapidly, fake news disguised as the media remains a major social problem. Fake news has become a particular public issue for the following reason:As online public opinion spheres are activated, people have diverse and easy access to platforms such as YouTube, and SNS. Unverified issues are produced and spread easily in that they can be freely spoken and shared information and opinions, making it easy for anyone to make news that may be extreme or wrongful, and spread them. Since 2017, academic research and discussion have gotten in earnest in the United States, where fake news remarked by Trump had been shifted to fake news.<a name=\"holannote\"></a>[<sup>1</sup>](#holan)According to previous discussions of fake news, fake news has been broadly defined as A) for political and economic gain, B) deliberately distorting and fabricating, and C) false information disguised as media coverage.<a name=\"westnote\"></a>[<sup>2</sup>](#west)With the introduction of artificial intelligence and various technology to distinguish fake news and unidentified information by SNS, media companies, also with the emergence of several fact-checking media show that media literacy has become a must for modern people.<a name=\"arxivnote\"></a>[<sup>3</sup>](#arxiv) Machine Learning(ML) algorithm can be the solution. At first, the use of ML algorithm can detect fake news easily and automatically. In addition, the dataset can easily be collected to train calssifiers, especially supervised calssifiers. Since these classifiers depend on labelled datasets, by training these classifiers, we can identify fake news more accurately than from a human perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Fake news damages certain people's reputation, violates someone’s privacy and personality rights and causes false prejudices against specific groups. Furthermore, it reduces the credibility of established media and other opinions and causes distortions in the process of forming political opinions. This causes social confusion and division, which harms civil society and its members and society as a whole. This is because it can cover facts and truths and spread false or distorted information to hinder the formation of sound public opinion. Naturally, such crackdowns and regulations on fake news are necessary. However, the biggest problem with fake news that we faced is that the current concept of fake news can be widespread and ambiguous. This is why the priority should be to find out what fake news is if legislation is to save damage to fake news. In a world of millions of news, they rely on manual human detection, so their scope is so limited that if the fake news is posted and deleted every minute, they cannot be manually responsible or executed. It can be a solution through the development of a system that provides reliable, automated exponential scores, namely trained machine learning.  solution through the development of a system that provides reliable, automated exponential scores, namely trained machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erase later\n",
    "\n",
    "# Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "# If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "# Please give the following infomration for each dataset you are using\n",
    "# - link/reference to obtain it\n",
    "# - description of the size of the dataset (# of variables, # of observations)\n",
    "# - what an observation consists of\n",
    "# - what some critical variables are, how they are represented\n",
    "# - any special handling, transformations, cleaning, etc you have done should be demonstrated here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset1: \n",
    "The data source used for this project is LIAR dataset which contains 3 files with .tsv format for test, train and validation. Below is some description about the data files used for this project.\n",
    "\n",
    "##### LIAR: A BENCHMARK DATASET FOR FAKE NEWS DETECTION\n",
    "\n",
    "William Yang Wang, \"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection, to appear in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), short paper, Vancouver, BC, Canada, July 30-August 4, ACL.\n",
    "\n",
    "original dataset contained 13 variables/columns for train, test and validation sets as follows:\n",
    "\n",
    "`Column 1`: the ID of the statement ([ID].json)\n",
    "\n",
    "`Column 2`: the label (Label class contains: True, Mostly-true, Half-true, Barely-true, FALSE, Pants-fire)\n",
    "\n",
    "`Column 3`: the statement\n",
    "\n",
    "`Column 4`: the subject(s)\n",
    "\n",
    "`Column 5`: the speaker\n",
    "\n",
    "`Column 6`: the speaker's job title\n",
    "\n",
    "`Column 7`: the state info\n",
    "\n",
    "`Column 8`: the party affiliation\n",
    "\n",
    " : For `Column 3`, we used countervectorize for statement, and `Column 4 to 8`, they are all discrete variables, so we didn’t consider to train this variables\n",
    "\n",
    "\n",
    "`Column 9-13`: the total credit history count, including the current statement.\n",
    "\n",
    "`Column 9`: barely true counts\n",
    "\n",
    "`Column 10`: false counts\n",
    "\n",
    "`Column 11`: half true counts\n",
    "\n",
    "`Column 12`: mostly true counts\n",
    "\n",
    "`Column 13`: pants on fire counts\n",
    "\n",
    "`Column 14`: the context (venue / location of the speech or statement)\n",
    "\n",
    "To make things simple we have chosen only 2 variables from this original dataset for this classification. The other variables can be added later to add some more complexity and enhance the features.\n",
    "\n",
    "Below are the columns used to create 3 datasets that have been in used in this project\n",
    "\n",
    "`Column 1`: Statement (News headline or text)\n",
    "\n",
    "`Column 2`: Label (Label class contains: True, False)\n",
    "\n",
    "newly created dataset has only 2 classes as compared to 6 from original classes. Below is method used for reducing the number of classes.\n",
    "\n",
    "* Original -- \tNew\n",
    "\n",
    "* True -- \t\tTrue\n",
    "\n",
    "* Mostly-true --  True\n",
    "\n",
    "* Half-true -- \t True\n",
    "\n",
    "* Barely-true --   False\n",
    "\n",
    "* False --\t\tFalse\n",
    "\n",
    "* Pants-fire --\tFalse\n",
    "\n",
    "\n",
    "\n",
    "## DATASET 2:\n",
    "\n",
    "##### FAKE NEWS NET Dataset\n",
    "\n",
    "* politifact_fake.csv - Samples related to fake news collected from PolitiFact\n",
    "\n",
    "* politifact_real.csv - Samples related to real news collected from PolitiFact\n",
    "\n",
    "* gossipcop_fake.csv - Samples related to fake news collected from GossipCop\n",
    "\n",
    "* gossipcop_real.csv - Samples related to real news collected from GossipCop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Each of the above CSV files is comma separated file and have the following columns\n",
    "\n",
    "\n",
    "`id` - Unique identifider for each news\n",
    "\n",
    "`url` - Url of the article from web that published that news\n",
    "\n",
    "`title` - Title of the news article\n",
    "\n",
    "`tweet_ids` - Tweet ids of tweets sharing news. This field is list of tweetids separated by tab.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LIAR dataset and the politifact and gossipcop datasets all contain information related to news and its veracity. They can be combined to create a larger dataset that can be used for training models to detect fake news. Use one dataset to validate the other: two datasets contain different types of information, they can be used to validate each other: LIAR dataset can be used to validate the performance of models trained on the politifact and gossipcop datasets, and vice versa. We can use one dataset for feature engineering:The politifact and gossipcop datasets can be used to extract features related to the sources of news articles, which can be used to train models on the LIAR dataset. Also, this shows using ensemble methods: Ensemble methods combine the predictions of multiple models to improve accuracy. By training models on both datasets and using ensemble methods, the overall performance of the model can be improved. In summary, combining datasets, using one dataset for feature engineering, and using ensemble methods can all be effective ways to use our two datasets. The most effective approach will depend on the specific research question and the nature of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = './liar_dataset/test.tsv'\n",
    "train_filename = './liar_dataset/train.tsv'\n",
    "valid_filename = './liar_dataset/valid.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = ['ID.json','label','statement','subject','speaker',\"speaker's job title\",'state','party affiliation','barely true counts','false counts','half true counts','mostly true counts','pants on fore counts','the context']\n",
    "train_data_origin = pd.read_csv(train_filename, sep=\"\\t\")\n",
    "test_data_origin = pd.read_csv(test_filename, sep=\"\\t\")\n",
    "valid_data_origin = pd.read_csv(valid_filename, sep=\"\\t\")\n",
    "\n",
    "train_data = train_data_origin.values.tolist()\n",
    "test_data = test_data_origin.values.tolist()\n",
    "valid_data = valid_data_origin.values.tolist()\n",
    "\n",
    "train_data = pd.DataFrame(train_data, columns=column_name)\n",
    "test_data = pd.DataFrame(test_data, columns=column_name)\n",
    "valid_data = pd.DataFrame(valid_data, columns=column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID.json</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker's job title</th>\n",
       "      <th>state</th>\n",
       "      <th>party affiliation</th>\n",
       "      <th>barely true counts</th>\n",
       "      <th>false counts</th>\n",
       "      <th>half true counts</th>\n",
       "      <th>mostly true counts</th>\n",
       "      <th>pants on fore counts</th>\n",
       "      <th>the context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12465.json</td>\n",
       "      <td>true</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>education</td>\n",
       "      <td>robin-vos</td>\n",
       "      <td>Wisconsin Assembly speaker</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a an online opinion-piece</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID.json        label                                          statement  \\\n",
       "0  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "1    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "2   1123.json        false  Health care reform legislation is likely to ma...   \n",
       "3   9028.json    half-true  The economic turnaround started at the end of ...   \n",
       "4  12465.json         true  The Chicago Bears have had more starting quart...   \n",
       "\n",
       "                              subject         speaker  \\\n",
       "0  energy,history,job-accomplishments  scott-surovell   \n",
       "1                      foreign-policy    barack-obama   \n",
       "2                         health-care    blog-posting   \n",
       "3                        economy,jobs   charlie-crist   \n",
       "4                           education       robin-vos   \n",
       "\n",
       "          speaker's job title      state party affiliation  \\\n",
       "0              State delegate   Virginia          democrat   \n",
       "1                   President   Illinois          democrat   \n",
       "2                         NaN        NaN              none   \n",
       "3                         NaN    Florida          democrat   \n",
       "4  Wisconsin Assembly speaker  Wisconsin        republican   \n",
       "\n",
       "   barely true counts  false counts  half true counts  mostly true counts  \\\n",
       "0                 0.0           0.0               1.0                 1.0   \n",
       "1                70.0          71.0             160.0               163.0   \n",
       "2                 7.0          19.0               3.0                 5.0   \n",
       "3                15.0           9.0              20.0                19.0   \n",
       "4                 0.0           3.0               2.0                 5.0   \n",
       "\n",
       "   pants on fore counts                the context  \n",
       "0                   0.0            a floor speech.  \n",
       "1                   9.0                     Denver  \n",
       "2                  44.0             a news release  \n",
       "3                   2.0        an interview on CNN  \n",
       "4                   1.0  a an online opinion-piece  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID.json</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker's job title</th>\n",
       "      <th>state</th>\n",
       "      <th>party affiliation</th>\n",
       "      <th>barely true counts</th>\n",
       "      <th>false counts</th>\n",
       "      <th>half true counts</th>\n",
       "      <th>mostly true counts</th>\n",
       "      <th>pants on fore counts</th>\n",
       "      <th>the context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11685.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Wisconsin is on pace to double the number of l...</td>\n",
       "      <td>jobs</td>\n",
       "      <td>katrina-shankland</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a news conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11096.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says John McCain has done nothing to help the ...</td>\n",
       "      <td>military,veterans,voting-record</td>\n",
       "      <td>donald-trump</td>\n",
       "      <td>President-Elect</td>\n",
       "      <td>New York</td>\n",
       "      <td>republican</td>\n",
       "      <td>63</td>\n",
       "      <td>114</td>\n",
       "      <td>51</td>\n",
       "      <td>37</td>\n",
       "      <td>61</td>\n",
       "      <td>comments on ABC's This Week.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5209.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
       "      <td>medicare,message-machine-2012,campaign-adverti...</td>\n",
       "      <td>rob-cornilles</td>\n",
       "      <td>consultant</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>republican</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a radio show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9524.json</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>When asked by a reporter whether hes at the ce...</td>\n",
       "      <td>campaign-finance,legal-issues,campaign-adverti...</td>\n",
       "      <td>state-democratic-party-wisconsin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>democrat</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>a web video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5962.json</td>\n",
       "      <td>true</td>\n",
       "      <td>Over the past five years the federal governmen...</td>\n",
       "      <td>federal-budget,pensions,retirement</td>\n",
       "      <td>brendan-doherty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>republican</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a campaign website</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID.json       label                                          statement  \\\n",
       "0  11685.json       false  Wisconsin is on pace to double the number of l...   \n",
       "1  11096.json       false  Says John McCain has done nothing to help the ...   \n",
       "2   5209.json   half-true  Suzanne Bonamici supports a plan that will cut...   \n",
       "3   9524.json  pants-fire  When asked by a reporter whether hes at the ce...   \n",
       "4   5962.json        true  Over the past five years the federal governmen...   \n",
       "\n",
       "                                             subject  \\\n",
       "0                                               jobs   \n",
       "1                    military,veterans,voting-record   \n",
       "2  medicare,message-machine-2012,campaign-adverti...   \n",
       "3  campaign-finance,legal-issues,campaign-adverti...   \n",
       "4                 federal-budget,pensions,retirement   \n",
       "\n",
       "                            speaker   speaker's job title         state  \\\n",
       "0                 katrina-shankland  State representative     Wisconsin   \n",
       "1                      donald-trump       President-Elect      New York   \n",
       "2                     rob-cornilles            consultant        Oregon   \n",
       "3  state-democratic-party-wisconsin                   NaN     Wisconsin   \n",
       "4                   brendan-doherty                   NaN  Rhode Island   \n",
       "\n",
       "  party affiliation  barely true counts  false counts  half true counts  \\\n",
       "0          democrat                   2             1                 0   \n",
       "1        republican                  63           114                51   \n",
       "2        republican                   1             1                 3   \n",
       "3          democrat                   5             7                 2   \n",
       "4        republican                   1             2                 1   \n",
       "\n",
       "   mostly true counts  pants on fore counts                   the context  \n",
       "0                   0                     0             a news conference  \n",
       "1                  37                    61  comments on ABC's This Week.  \n",
       "2                   1                     1                  a radio show  \n",
       "3                   2                     7                   a web video  \n",
       "4                   1                     0            a campaign website  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID.json</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker's job title</th>\n",
       "      <th>state</th>\n",
       "      <th>party affiliation</th>\n",
       "      <th>barely true counts</th>\n",
       "      <th>false counts</th>\n",
       "      <th>half true counts</th>\n",
       "      <th>mostly true counts</th>\n",
       "      <th>pants on fore counts</th>\n",
       "      <th>the context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>238.json</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>When Obama was sworn into office, he DID NOT u...</td>\n",
       "      <td>obama-birth-certificate,religion</td>\n",
       "      <td>chain-email</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7891.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says Having organizations parading as being so...</td>\n",
       "      <td>campaign-finance,congress,taxes</td>\n",
       "      <td>earl-blumenauer</td>\n",
       "      <td>U.S. representative</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a U.S. Ways and Means hearing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8169.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Says nearly half of Oregons children are poor.</td>\n",
       "      <td>poverty</td>\n",
       "      <td>jim-francesconi</td>\n",
       "      <td>Member of the State Board of Higher Education</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>an opinion article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>929.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>On attacks by Republicans that various program...</td>\n",
       "      <td>economy,stimulus</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>160</td>\n",
       "      <td>163</td>\n",
       "      <td>9</td>\n",
       "      <td>interview with CBS News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9416.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says when armed civilians stop mass shootings ...</td>\n",
       "      <td>guns</td>\n",
       "      <td>jim-rubens</td>\n",
       "      <td>Small business owner</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>republican</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>in an interview at gun shop in Hudson, N.H.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID.json       label                                          statement  \\\n",
       "0   238.json  pants-fire  When Obama was sworn into office, he DID NOT u...   \n",
       "1  7891.json       false  Says Having organizations parading as being so...   \n",
       "2  8169.json   half-true     Says nearly half of Oregons children are poor.   \n",
       "3   929.json   half-true  On attacks by Republicans that various program...   \n",
       "4  9416.json       false  Says when armed civilians stop mass shootings ...   \n",
       "\n",
       "                            subject          speaker  \\\n",
       "0  obama-birth-certificate,religion      chain-email   \n",
       "1   campaign-finance,congress,taxes  earl-blumenauer   \n",
       "2                           poverty  jim-francesconi   \n",
       "3                  economy,stimulus     barack-obama   \n",
       "4                              guns       jim-rubens   \n",
       "\n",
       "                             speaker's job title          state  \\\n",
       "0                                            NaN            NaN   \n",
       "1                            U.S. representative         Oregon   \n",
       "2  Member of the State Board of Higher Education         Oregon   \n",
       "3                                      President       Illinois   \n",
       "4                           Small business owner  New Hampshire   \n",
       "\n",
       "  party affiliation  barely true counts  false counts  half true counts  \\\n",
       "0              none                  11            43                 8   \n",
       "1          democrat                   0             1                 1   \n",
       "2              none                   0             1                 1   \n",
       "3          democrat                  70            71               160   \n",
       "4        republican                   1             1                 0   \n",
       "\n",
       "   mostly true counts  pants on fore counts  \\\n",
       "0                   5                   105   \n",
       "1                   1                     0   \n",
       "2                   1                     0   \n",
       "3                 163                     9   \n",
       "4                   1                     0   \n",
       "\n",
       "                                   the context  \n",
       "0                                          NaN  \n",
       "1                a U.S. Ways and Means hearing  \n",
       "2                           an opinion article  \n",
       "3                      interview with CBS News  \n",
       "4  in an interview at gun shop in Hudson, N.H.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label and statement\n",
    "train_news = train_data.iloc[:,1:3]\n",
    "test_news = test_data.iloc[:,1:3]\n",
    "valid_news = test_data.iloc[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>true</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                          statement\n",
       "0    half-true  When did the decline of coal start? It started...\n",
       "1  mostly-true  Hillary Clinton agrees with John McCain \"by vo...\n",
       "2        false  Health care reform legislation is likely to ma...\n",
       "3    half-true  The economic turnaround started at the end of ...\n",
       "4         true  The Chicago Bears have had more starting quart..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "half-true      2114\n",
      "false          1994\n",
      "mostly-true    1962\n",
      "true           1676\n",
      "barely-true    1654\n",
      "pants-fire      839\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check the unique value of label\n",
    "print(train_news['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace original label to binary label\n",
    "train_news = train_news.replace({'label' : { 'half-true' : 1, 'false' : 0, 'mostly-true' : 1 , 'true' : 1, 'barely-true' : 0, 'pants-fire' : 0}})\n",
    "test_news = test_news.replace({'label' : { 'half-true' : 1, 'false' : 0, 'mostly-true' : 1 , 'true' : 1, 'barely-true' : 0, 'pants-fire' : 0}})\n",
    "valid_news = valid_news.replace({'label' : { 'half-true' : 1, 'false' : 0, 'mostly-true' : 1 , 'true' : 1, 'barely-true' : 0, 'pants-fire' : 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          statement\n",
       "0      1  When did the decline of coal start? It started...\n",
       "1      1  Hillary Clinton agrees with John McCain \"by vo...\n",
       "2      0  Health care reform legislation is likely to ma...\n",
       "3      1  The economic turnaround started at the end of ...\n",
       "4      1  The Chicago Bears have had more starting quart..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    5752\n",
      "0    4487\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_news['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact_fake = pd.read_csv('./FakeNewsNet/politifact_fake.csv')\n",
    "politifact_fake['label'] = 'fake'\n",
    "politifact_real = pd.read_csv('./FakeNewsNet/politifact_real.csv')\n",
    "politifact_real['label'] = 'real'\n",
    "gossipcop_fake = pd.read_csv('./FakeNewsNet/gossipcop_fake.csv')\n",
    "gossipcop_fake['label'] = 'fake'\n",
    "gossipcop_real = pd.read_csv('./FakeNewsNet/gossipcop_real.csv')\n",
    "gossipcop_real['label'] = 'real'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = pd.concat([politifact_fake, gossipcop_fake])\n",
    "real_news = pd.concat([politifact_real, gossipcop_real])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribution(dataFile):\n",
    "    return sb.countplot(x='label', data=dataFile, palette='hls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#by calling below we can see that training, test and valid data seems to be failry evenly distributed between the classes\n",
    "create_distribution(train_news)\n",
    "create_distribution(test_news)\n",
    "create_distribution(valid_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing label\n",
    "def data_qualityCheck():\n",
    "    print('train data')\n",
    "    print(train_news.isnull().sum())\n",
    "    train_news.info()\n",
    "    print('\\n')\n",
    "        \n",
    "    print('test data')\n",
    "    print(test_news.isnull().sum())\n",
    "    test_news.info()\n",
    "    print('\\n')\n",
    "\n",
    "    print('valid data')\n",
    "    print(valid_news.isnull().sum())\n",
    "    valid_news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "label        0\n",
      "statement    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10239 entries, 0 to 10238\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   label      10239 non-null  int64 \n",
      " 1   statement  10239 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 160.1+ KB\n",
      "\n",
      "\n",
      "test data\n",
      "label        0\n",
      "statement    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1266 entries, 0 to 1265\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   label      1266 non-null   int64 \n",
      " 1   statement  1266 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 19.9+ KB\n",
      "\n",
      "\n",
      "valid data\n",
      "label        0\n",
      "statement    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1266 entries, 0 to 1265\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   label      1266 non-null   int64 \n",
      " 1   statement  1266 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 19.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data_qualityCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10239, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_news.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1266, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_news.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The evaluation metrics we will be using will be True positive rate (recall) and Accuracy. For recall, we will be using (correctly evaluating that news is false) / (correctly evaluating that news is false + evaluating that news is true when it is actually false). This recall rate is important for our problem because it tells us the rate of us being able to correctly evaluate that the news is false. For Accuracy, we will be using (correctly evaluating that news is false + evaluating that news is false when it is actually true) / (correctly evaluating that news is false + correctly evaluating that news is true + evaluating that news is true when it is actually false + evaluating that news is false when it is actually true). Accuracy is important for our problem because it tells us if our model was able to accurately make correct predictions in evaluating that the news is false. We will then plot an ROC curve to show the performance of the classification model and then find the AUC to provide an aggregate measure of performance across all possible classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "##### Benchmark model: \n",
    "BERT (Bidirectional Encoder Representations from Transformers), FakeNewsNet: https://arxiv.org/pdf/1809.01286.pdf\n",
    "\n",
    "##### Solution: multivariable logistic regression\n",
    "\n",
    "##### Library used: numpy, pandas, scikit-learn\n",
    "\n",
    "Since our problem is a multiclass classification of evaluating if the news is True, Mostly-true, Half-true, Barely-true, FALSE, Pants-fire (6 labels), we will be using multi-class logistic regression. In order to process the test into input data, we will be using vectorization. Then we will be adding this vectorized input to variables from multiple columns and evaluate whether the news is true or false. Thus, we will be using multivariable logistic regression. First of all, we will be dividing the dataset into training/validation set and test set. Next, because vectorizing the text into word units creates too many variables, we will be using normalization. Because we will be needing strong normalization, we believe that using L1 normalization will be effective. The search plane itself is a vectorized text, therefore it is possible for it to become very high dimension which may result in being stuck in the local minima. In order to prevent this problem, we will be using momentum in our training. Also, in order to quickly do weight updates in training, we will be using mini-batch. In the validation step, we will be using K-fold cross validation to retreive the most accurate model. \n",
    "\n",
    "##### Text Processing\n",
    "After converting articles into token lists through CountVectorizer, TfidTransformer, and TfidVectorizer, vectorize them based on the count and frequency of appearance of tokens. In addition, TfidVectorier creates a BOW encoding vector that weights words in a TF-IDF, enabling document preprocessing. Using the TF-IDF method, you can manually adjust the parameter C to use an estimate that is a LogisticRegressionCV. Specifying cv=5, using kfold cross-validation for hyperparameter tuning. The criterion for model measurement is the accuracy of classification, and by setting it to n_jobs=-1, all CPU cores are dedicated. We’ll evaluate the performance by maximizing the number of iterations of the optimization algorithm.\n",
    "\n",
    "##### Modeling\n",
    "As a classification model, machine learning algorithms can be used \n",
    "a) Logistic Registration, b) SVM, c) Random Forest, and boosted trees(AdaBoost)\n",
    "\n",
    "First, modeling using the logistic regression algorithm.\n",
    "For modeling, separate train, validation sets and test set.\n",
    "Using the same dataset and the same text method and train_test_split, we can create logistic regression models that classify news as real or fake news. After organizing and preprocessing text data, performing feature extraction, and building and distributing a logistic regression classifier using the scikit-learn, evaluate the accuracy of the model.\n",
    "Second, Using SVM(Support Vector Machines) to build classification prediction models and compare prediction accuracy on datasets for verification.\n",
    "Third, tree-based random forest. Using Adaboost to fit the shallow decision tree on the train set, and additional classifiers can be placed on the same data to correct errors and can be weighted. \n",
    "Also, k-fold cross-validation, which is known to be useful because the amount of data is not enough to experiment with a larger amount of data on a smaller dataset to increase the statistical reliability of methodological performance measurements.\n",
    "\n",
    "##### Evaluation\n",
    "Confusion matrix : f1_score, report / Precision, recall \n",
    "\n",
    "\n",
    "##### What we see moving forward and limitations\n",
    "Using RNN, CNN, and LSTM, which are algorithms of deep learning could have analyzed with better performance. However, we need to review whether there are enough samples for deep learning, and we can later create a model using appropriate algorithms by devising how to classify new articles that we have never seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yewonhong/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "countV = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "train_count = countV.fit_transform(train_news['statement'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10239, 12060)\n"
     ]
    }
   ],
   "source": [
    "print(train_count.shape)\n",
    "# (# of statement, # of word(all word in every statement) in statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3158)\t1\n",
      "  (0, 2449)\t1\n",
      "  (0, 10330)\t1\n",
      "  (0, 10331)\t2\n",
      "  (0, 7347)\t1\n",
      "  (0, 4819)\t1\n",
      "  (0, 11027)\t1\n",
      "  (0, 1513)\t1\n",
      "  (0, 8466)\t1\n",
      "  (0, 4869)\t1\n",
      "  (0, 1941)\t1\n",
      "  (0, 749)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_count[0])\n",
    "# print(countV.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfV = TfidfVectorizer(use_idf = True,stop_words = stopwords.words('english'),ngram_range=(1, 2), max_df=300)\n",
    "train_tfidf = tfidfV.fit_transform(train_news['statement'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10239, 85862)\n"
     ]
    }
   ],
   "source": [
    "print(train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12450)\t0.22202408693881748\n",
      "  (0, 31951)\t0.19903085986550703\n",
      "  (0, 58219)\t0.18041282385564963\n",
      "  (0, 9578)\t0.2394177917569828\n",
      "  (0, 71745)\t0.2394177917569828\n",
      "  (0, 77780)\t0.2292431266897432\n",
      "  (0, 31666)\t0.2394177917569828\n",
      "  (0, 49524)\t0.18571537445922032\n",
      "  (0, 71759)\t0.2394177917569828\n",
      "  (0, 71735)\t0.2394177917569828\n",
      "  (0, 16251)\t0.2394177917569828\n",
      "  (0, 21471)\t0.2394177917569828\n",
      "  (0, 4550)\t0.1302527364495592\n",
      "  (0, 12448)\t0.18723667730248678\n",
      "  (0, 31946)\t0.14445804970744705\n",
      "  (0, 9572)\t0.1944557170534125\n",
      "  (0, 77723)\t0.13107555808862625\n",
      "  (0, 31630)\t0.14532844256774455\n",
      "  (0, 49521)\t0.18041282385564963\n",
      "  (0, 71739)\t0.3260382380749686\n",
      "  (0, 71714)\t0.17319378410472394\n",
      "  (0, 16235)\t0.17812999518480985\n",
      "  (0, 21468)\t0.20167475680433822\n"
     ]
    }
   ],
   "source": [
    "print(train_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token = []\n",
    "for sentence in train_news['statement'].values:\n",
    "    token = list(sentence.lower().split())\n",
    "    word_token.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/sch32ylx6b541jfnlv91jrrr0000gn/T/ipykernel_79386/2640026966.py:2: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences=word_token)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0752309  -0.09773006 -0.08251093  0.0284546  -0.13054967 -0.0117858\n",
      "  0.14333105  0.2582307  -0.20216896 -0.08443466 -0.04803344 -0.1506896\n",
      "  0.09828881  0.13373812 -0.05662246  0.04375344  0.04608474 -0.13572657\n",
      " -0.01156373  0.01799692  0.08672985 -0.11526693 -0.02327009 -0.14417426\n",
      "  0.02505136 -0.0844653  -0.01388858 -0.10095975 -0.19784875 -0.01923068\n",
      "  0.0992981   0.00781605 -0.06464487  0.04368939  0.14710155  0.03621343\n",
      "  0.08783379 -0.06206631  0.00163489 -0.06602458  0.05362232 -0.14824371\n",
      " -0.13980688  0.14793976  0.19266772 -0.05590713 -0.09180738  0.02421123\n",
      " -0.08709449  0.16758662  0.14617302 -0.06114415 -0.16228029 -0.02402788\n",
      "  0.04921686  0.08193208 -0.00898401  0.1056025  -0.17345859 -0.07141698\n",
      "  0.05746529  0.05630973 -0.04109546 -0.02355005 -0.12603347 -0.01498333\n",
      "  0.06480046  0.18463495  0.01705693  0.1783599  -0.02857157 -0.13476782\n",
      "  0.0062136   0.02676015  0.03685028  0.13807902  0.02991689  0.04241162\n",
      "  0.06501901  0.00490718 -0.12203413 -0.07833581 -0.04916079  0.2139965\n",
      "  0.03167969  0.01966067  0.00754868  0.05291529  0.15006444 -0.0700869\n",
      "  0.06003208  0.18747745  0.02772953  0.05730175  0.12463176 -0.0762337\n",
      "  0.06660953 -0.04246248  0.13049133  0.04410795]\n"
     ]
    }
   ],
   "source": [
    "word2vec = model.wv\n",
    "print(word2vec.get_vector('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 0.3455028831958771),\n",
       " ('be', 0.30906975269317627),\n",
       " ('they', 0.29532891511917114),\n",
       " ('if', 0.29445165395736694),\n",
       " ('get', 0.26555967330932617),\n",
       " ('floor', 0.26008927822113037),\n",
       " ('defend', 0.25121793150901794),\n",
       " ('to', 0.23744744062423706),\n",
       " ('dont', 0.23166143894195557),\n",
       " ('drink', 0.2281871885061264)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('coal','decline') \n",
    "# model.similarity('coal','decline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA does not support sparse input. See TruncatedSVD for a possible alternative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c_/sch32ylx6b541jfnlv91jrrr0000gn/T/ipykernel_79386/3644165503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \"\"\"\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;31m# This is more informative than the generic one raised by check_array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0;34m\"PCA does not support sparse input. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0;34m\"TruncatedSVD for a possible alternative.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA does not support sparse input. See TruncatedSVD for a possible alternative."
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import_ipynb in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: IPython in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from import_ipynb) (7.31.1)\n",
      "Requirement already satisfied: nbformat in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from import_ipynb) (5.5.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (3.0.20)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (0.1.6)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (4.8.0)\n",
      "Requirement already satisfied: backcall in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (63.4.1)\n",
      "Requirement already satisfied: decorator in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (0.7.5)\n",
      "Requirement already satisfied: appnope in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (0.1.2)\n",
      "Requirement already satisfied: pygments in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (2.11.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from IPython->import_ipynb) (0.18.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from nbformat->import_ipynb) (4.16.0)\n",
      "Requirement already satisfied: fastjsonschema in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from nbformat->import_ipynb) (2.16.2)\n",
      "Requirement already satisfied: jupyter_core in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from nbformat->import_ipynb) (4.11.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.18.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.5)\n",
      "Requirement already satisfied: sklearn_evaluation in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (0.11.4)\n",
      "Requirement already satisfied: pandas in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (1.4.4)\n",
      "Requirement already satisfied: decorator in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (5.1.1)\n",
      "Requirement already satisfied: mistune in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (0.8.4)\n",
      "Requirement already satisfied: parso in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (0.8.3)\n",
      "Requirement already satisfied: ipython in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (7.31.1)\n",
      "Requirement already satisfied: tabulate in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (0.8.10)\n",
      "Requirement already satisfied: black in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (22.6.0)\n",
      "Requirement already satisfied: ploomber-core>=0.2.6 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (0.2.6)\n",
      "Requirement already satisfied: matplotlib in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (3.5.2)\n",
      "Requirement already satisfied: nbformat in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (5.5.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (1.0.2)\n",
      "Requirement already satisfied: jinja2 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from sklearn_evaluation) (2.11.3)\n",
      "Requirement already satisfied: posthog in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ploomber-core>=0.2.6->sklearn_evaluation) (2.4.0)\n",
      "Requirement already satisfied: click in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ploomber-core>=0.2.6->sklearn_evaluation) (8.0.4)\n",
      "Requirement already satisfied: pyyaml in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ploomber-core>=0.2.6->sklearn_evaluation) (6.0)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from black->sklearn_evaluation) (2.5.2)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from black->sklearn_evaluation) (2.0.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from black->sklearn_evaluation) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from black->sklearn_evaluation) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from black->sklearn_evaluation) (4.3.0)\n",
      "Requirement already satisfied: backcall in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (0.1.6)\n",
      "Requirement already satisfied: appnope in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (0.1.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (63.4.1)\n",
      "Requirement already satisfied: pickleshare in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (3.0.20)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (4.8.0)\n",
      "Requirement already satisfied: pygments in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from ipython->sklearn_evaluation) (2.11.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from jinja2->sklearn_evaluation) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->sklearn_evaluation) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->sklearn_evaluation) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->sklearn_evaluation) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->sklearn_evaluation) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->sklearn_evaluation) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->sklearn_evaluation) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->sklearn_evaluation) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->sklearn_evaluation) (1.21.5)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from nbformat->sklearn_evaluation) (4.16.0)\n",
      "Requirement already satisfied: fastjsonschema in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from nbformat->sklearn_evaluation) (2.16.2)\n",
      "Requirement already satisfied: jupyter_core in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from nbformat->sklearn_evaluation) (4.11.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from pandas->sklearn_evaluation) (2022.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn_evaluation) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn_evaluation) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn_evaluation) (2.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->sklearn_evaluation) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->sklearn_evaluation) (0.18.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython->sklearn_evaluation) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->sklearn_evaluation) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->sklearn_evaluation) (1.16.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from posthog->ploomber-core>=0.2.6->sklearn_evaluation) (2.2.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from posthog->ploomber-core>=0.2.6->sklearn_evaluation) (1.6)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from posthog->ploomber-core>=0.2.6->sklearn_evaluation) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn_evaluation) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn_evaluation) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn_evaluation) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yewonhong/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn_evaluation) (3.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install import_ipynb\n",
    "! pip install sklearn_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb #Annotate later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Feature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix, roc_curve, roc_auc_score\n",
    "import sklearn_evaluation\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ^^이거 위에 마지막에 내가 지울게 다돌리고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train, test, validation set\n",
    "X_train = train_news['statement']\n",
    "y_train = train_news['label']\n",
    "\n",
    "X_test = test_news['statement']\n",
    "y_test = test_news['label']\n",
    "\n",
    "X_val = valid_news['statement']\n",
    "y_val = valid_news['label']\n",
    "\n",
    "# define whole dataset\n",
    "X = pd.concat([X_train, X_test, X_val])\n",
    "y = pd.concat([y_train, y_test, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classification function using model\n",
    "def classification(clf, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # using 5 fold cross validation\n",
    "    train_size,train_score,test_score = learning_curve(clf, X_train, y_train, cv=5)\n",
    "    sklearn_evaluation.plot.learning_curve(train_score, test_score, train_size)\n",
    "    \n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    clf_table = classification_report(y_test, y_pred, target_names=['false', 'true'])\n",
    "    \n",
    "    return acc, clf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(gridsearchcv):\n",
    "    params = gridsearchcv.cv_results_[\"params\"]\n",
    "    ys = gridsearchcv.cv_results_[\"mean_test_score\"]\n",
    "    xs = ['|'.join(str(v) for v in param.values()) for param in params]\n",
    "    yerr = gridsearchcv.cv_results_[\"std_test_score\"]\n",
    "    plt.errorbar(xs, ys, yerr / np.sqrt(gridsearchcv.cv), fmt='.k')\n",
    "    plt.ylabel(\"f1\")\n",
    "    plt.xlabel(\"params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgr = Pipeline([\n",
    "        ('feature',countV),\n",
    "        ('logistic',LogisticRegression(max_iter = 100))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'feature':[countV, tfidfV],\n",
    "    'logistic__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'logistic__penalty': ['l1', 'l2']\n",
    "}\n",
    "search_lgr = GridSearchCV(model_lgr, param_grid, scoring = 'f1_micro',cv=5)\n",
    "search_lgr.fit(X_train, y_train)\n",
    "print(search_lgr.best_params_)\n",
    "plot_results(search_lgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search_lgr.best_estimator_.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "clf_table = classification_report(y_test, y_pred, target_names=['false', 'true'])\n",
    "print(clf_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(roc_auc)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "print('Area under the Receiver Operating Characteristic curve:', \n",
    "      roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = Pipeline([\n",
    "        ('feature',tfidfV),\n",
    "        ('svm', SVC(probability=True))\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'feature':[countV, tfidfV],\n",
    "    'svm__kernel':['linear', 'rbf']\n",
    "}\n",
    "search_svm = GridSearchCV(model_svm, param_grid, scoring = 'f1_micro',cv=5)\n",
    "search_svm.fit(X_train, y_train)\n",
    "print(search_svm.best_params_)\n",
    "plot_results(search_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search_svm.best_estimator_.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "clf_table = classification_report(y_test, y_pred, target_names=['false', 'true'])\n",
    "print(clf_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(roc_auc)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "print('Area under the Receiver Operating Characteristic curve:', \n",
    "      roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = Pipeline([\n",
    "        ('feature',countV),\n",
    "        ('knn',KNeighborsClassifier())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'feature':[countV, tfidfV],\n",
    "    'knn__n_neighbors':[7,15,19]\n",
    "}\n",
    "search_knn = GridSearchCV(model_knn, param_grid, scoring = 'f1_micro',cv=5)\n",
    "search_knn.fit(X_train, y_train)\n",
    "print(search_knn.best_params_)\n",
    "plot_results(search_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search_knn.best_estimator_.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "clf_table = classification_report(y_test, y_pred, target_names=['false', 'true'])\n",
    "print(clf_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(roc_auc)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "print('Area under the Receiver Operating Characteristic curve:', \n",
    "      roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize model hyperparameters: Random Forest Classifier has several hyperparameters that can be tuned to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "model_rf = Pipeline([\n",
    "        ('feature',Feature.countV),\n",
    "        ('rf',RandomForestClassifier(n_jobs=3))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'feature':[countV, tfidfV],\n",
    "    'rf__n_estimators':[100,1000]\n",
    "}\n",
    "search_rf = GridSearchCV(model_rf, param_grid, scoring = 'f1_micro',cv=5)\n",
    "search_rf.fit(X_train, y_train)\n",
    "print(search_rf.best_params_)\n",
    "plot_results(search_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search_rf.best_estimator_.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "clf_table = classification_report(y_test, y_pred, target_names=['false', 'true'])\n",
    "print(clf_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(roc_auc)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "print('Area under the Receiver Operating Characteristic curve:', \n",
    "      roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Machine(GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbm = Pipeline([\n",
    "        ('feature',Feature.countV),\n",
    "        ('gbm',GradientBoostingClassifier(random_state=0))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'feature':[countV, tfidfV],\n",
    "}\n",
    "search_gbm = GridSearchCV(model_gbm, param_grid, scoring = 'f1_micro',cv=5)\n",
    "search_gbm.fit(X_train, y_train)\n",
    "print(search_gbm.best_params_)\n",
    "plot_results(search_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "clf_table = classification_report(y_test, y_pred, target_names=['false', 'true'])\n",
    "print(clf_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(roc_auc)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "print('Area under the Receiver Operating Characteristic curve:', \n",
    "      roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take each model with best hyperparameter according to grid search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr_best = Pipeline([\n",
    "        ('feature',tdidfV),\n",
    "        ('logistic',LogisticRegression(max_iter = 100, C =1.0, penalty='l2'))\n",
    "        ])\n",
    "svm_best = Pipeline([\n",
    "        ('feature',countV),\n",
    "        ('svm', SVC(probability=True, kernel='rbf'))\n",
    "         ])\n",
    "knn_best = Pipeline([\n",
    "        ('feature',tdidfV),\n",
    "        ('knn',KNeighborsClassifier(neighbors_n = 19))\n",
    "        ])\n",
    "rf_best = Pipeline([\n",
    "        ('feature',countV),\n",
    "        ('rf',RandomForestClassifier(n_estimators=1000, n_jobs=3))\n",
    "        ])\n",
    "gbm_best = Pipeline([\n",
    "        ('feature',countV),\n",
    "        ('gbm',GradientBoostingClassifier(random_state=0))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_model = VotingClassifier(\n",
    "    estimators=[('lgr',lgr_best),('svm',svm_best),('knn',knn_best),('rf',rf_best),('gbm',gbm_best)], # 3개의 약한 학습기\n",
    "    voting='hard'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "clf_table = classification_report(y_test, y_pred, target_names=['false', 'true'])\n",
    "print(clf_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(roc_auc)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "print('Area under the Receiver Operating Characteristic curve:', \n",
    "      roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics: While accuracy is a commonly used metric for classification tasks, it may not always be the best indicator of model performance, especially in cases where the class distribution is imbalanced. We considered using additional metrics such as precision, recall, F1-score, and ROC-AUC score to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanila logistic regression\n",
    "acc, table = classification(model_lgr_vanila, X, y)\n",
    "print(\"accuracy : \",acc)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic reguression with l2 regularization and less C\n",
    "acc, table = classification(model_lgr, X, y)\n",
    "print(\"accuracy : \",acc)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanila linear svm\n",
    "acc, table = classification(model_svm_vanila, X, y)\n",
    "print(\"accuracy : \",acc)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear svm with hinge loss\n",
    "acc, table = classification(model_svm, X, y)\n",
    "print(\"accuracy : \",acc)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "acc, table = classification(model_rf, X, y)\n",
    "print(\"accuracy : \",acc)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can consdier using an ensemble of models for better one. Ensemble methods such as bagging, boosting, and stacking can help improve the performance of the model by combining the predictions of multiple models. Consider using an ensemble of multiple Random Forest models or combining the Random Forest model with other models such as Logistic Regression, Naive Bayes, or Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이거 하면 좋을 거 같은데 시간 없으면 빼고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found Random Forest Classifier has the best training score and Cross-validation score, and accuracy, and the second better model is using vanila linear SVM. So, we concluded to choose final classifier model using Random Forest, and now, use input for looking for probability of truth as Fake News Detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "var = input(\"News: \")\n",
    "\n",
    "def detect_fake_news(var):    \n",
    "    fin_mod = pickle.load(open('final_model.sav', 'rb'))  // ???? \n",
    "    pred = fin_mod.predict([var])\n",
    "    prob = fin_mod.predict_proba([var])\n",
    "\n",
    "    return (print(\"Entered statement: \",pred[0]), print(\"Truth probablity: \",prob[0][1]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    detect_fake_news(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "Here we have build all the classifiers for predicting the fake news detection. The extracted features are fed into different classifiers. We have used Naive-bayes, Logistic Regression, Linear SVM, Stochastic gradient descent and Random forest classifiers from sklearn. Each of the extracted features were used in all of the classifiers. Once fitting the model, we compared the f1 score and checked the confusion matrix. After fitting all the classifiers, 2 best performing models were selected as candidate models for fake news classification. We have performed parameter tuning by implementing GridSearchCV methods on these candidate models and chosen best performing parameters for these classifier. Finally selected model was used for fake news detection with the probability of truth. In Addition to this, We have also extracted the top 50 features from our term-frequency tfidf vectorizer to see what words are most and important in each of the classes. We have also used Precision-Recall and learning curves to see how training and test set performs when we increase the amount of data in our classifiers.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Our team has endeavored to create a comprehensive and expansive data repository that addresses the limitations of existing datasets. We recognize that there are currently no public datasets available that can provide all possible features of news content, social context, and spatiotemporal information.\n",
    "\n",
    "Existing datasets, such as the LIAR dataset, have their own limitations. For instance, the LIAR dataset mostly consists of short statements rather than complete news articles with accompanying meta attributes. This lack of contextual information can limit the dataset's usefulness in certain research contexts.\n",
    "\n",
    "To address these limitations, we should design our data repository to include a diverse range of features and attributes that can capture the various facets of news content, social context, and spatiotemporal information. This includes not only the text of news articles but also the date, time, location, and other metadata associated with each article.\n",
    "\n",
    "Moreover, our data repository should be curated to ensure that it is representative of a wide range of news sources and perspectives. We recognize that the diversity of news sources and viewpoints is critical in promoting a healthy and informed public discourse. Therefore, we should make every effort to ensure that our data repository is inclusive of a wide range of sources and viewpoints.\n",
    "\n",
    "Overall, we believe that our data project represents a significant contribution to the field of media studies and can serve as a valuable resource for researchers and practitioners alike. By providing a more comprehensive and nuanced view of news content and context, we hope to foster a greater understanding of the media landscape and its impact on society.\n",
    "\n",
    " \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "n our dataset, we recognize that there may be potential ethical concerns that arise from retrieving data from previously published news articles. As our dataset aims to evaluate the veracity of news articles, there is a possibility that our findings may be construed as a criticism of the work of individual journalists or publishers. Such criticism could potentially harm the careers of those individuals, particularly if our findings are widely disseminated and negatively received.\n",
    "\n",
    "Furthermore, our use of someone else's work without their permission may raise privacy concerns, particularly if we are evaluating the work without the publisher's knowledge or consent. We recognize that these concerns are valid and that it is important to approach such issues with sensitivity and care.\n",
    "\n",
    "However, we believe that the potential benefits of our project outweigh these ethical concerns. It is an unfortunate reality that news itself is often biased or inaccurate, and it is the responsibility of readers to discern which sources they can trust. By providing a dataset that evaluates the veracity of news articles, we hope to empower readers to make more informed decisions and to foster a greater sense of media literacy.\n",
    "\n",
    "Moreover, we are committed to addressing ethical concerns in our dataset by ensuring that our methods are transparent, rigorous, and unbiased. We are mindful of the potential harm that our findings may cause to individual journalists and publishers and we will take every effort to ensure that our results are presented in a way that is fair and balanced.\n",
    "\n",
    "In conclusion, while we recognize that there are ethical concerns that arise from using previously published news articles in our dataset, we believe that the potential benefits of our project are significant. By providing a comprehensive and unbiased dataset, we hope to contribute to a more informed and media-literate society. We are committed to addressing ethical concerns in our work and will take every effort to ensure that our findings are presented in a way that is sensitive and responsible. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "\n",
    "Our goal for this project was to classify fake news using different evaluation methods. The methods we used was vanilla logistic regression, logistic regression with 12 regularization and less C, vanilla linear SVM, and linear SVM with hinge loss. Out of those methods, the best performing method was vanilla linear SVM having a consistent training score close to 1.00, cross-validation score of 0.65~0.70, false f1-score of 0.45, true f1-score of 0.79, and accuracy of 0.70. This result we found would fit in work fields such as journalism, politics and law enforcements. For journalism, fake news evaluation is crucial to ensure that the news reported is accurate and not misleading. For politics, Politicians and political parties can use fake news evaluation to fact-check information and avoid spreading false information. Finally, for law enforcemnts, fake news evaluation can be used by law enforcement agencies to identify false information being spread about criminal activities or investigations. For future work, we could extend our datasets by gathering data from other multiple fact-checking websites and data collections. Also we could introduce more feature selections to further increase our accuracy and performance of our models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
