{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Joshua Howon Kim\n",
    "- Yewon Hong\n",
    "- Seonghun Oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables\n",
    "\n",
    "As social media has become used from almost every person in the world, it has been easier for people to receive and share news. However, this easy access of news has also allowed it to be much easier to spread fake news. There are many platforms that purposely spread fake and biased news for their own benefit. It is very difficult for individuals to tell whether news is fake or real. Thus, this intentional spread of fake news is able to bring up many social conflicts, leading to disasters in the society. Therefore, it is very important for people to be able to correctly identify if the news they are reading is real or fake. The goal of our project is to solve this problem by using machine learning classifiers and correctly detecting if a news is real or fake. The data we are using is scraped from snopes.com which is a website that fact checks and analyzes information from the news. The variables in the dataset include: the title, comments by the public on the fact, claims to support the comment, label of the news (True, False, Miscaptioned), summary of the content, information that are true, false and unknown. We will be using these data to understand how each variable affects on the validity of the news content. In order to validate if the news is fake or not, we will be using multivariable logistic regression and SVM.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "As SNS spreads rapidly, fake news disguised as the media remains a major social problem. Fake news has become a particular public issue for the following reason:As online public opinion spheres are activated, people have diverse and easy access to platforms such as YouTube, and SNS. Unverified issues are produced and spread easily in that they can be freely spoken and shared information and opinions, making it easy for anyone to make news that may be extreme or wrongful, and spread them. Since 2017, academic research and discussion have gotten in earnest in the United States, where fake news remarked by Trump had been shifted to fake news.<a name=\"holannote\"></a>[<sup>1</sup>](#holan)According to previous discussions of fake news, fake news has been broadly defined as A) for political and economic gain, B) deliberately distorting and fabricating, and C) false information disguised as media coverage.<a name=\"westnote\"></a>[<sup>2</sup>](#west)With the introduction of artificial intelligence and various technology to distinguish fake news and unidentified information by SNS, media companies, also with the emergence of several fact-checking media show that media literacy has become a must for modern people.<a name=\"arxivnote\"></a>[<sup>3</sup>](#arxiv) Machine Learning(ML) algorithm can be the solution. At first, the use of ML algorithm can detect fake news easily and automatically. In addition, the dataset can easily be collected to train calssifiers, especially supervised calssifiers. Since these classifiers depend on labelled datasets, by training these classifiers, we can identify fake news more accurately than from a human perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Fake news damages certain people's reputation, violates someone’s privacy and personality rights and causes false prejudices against specific groups. Furthermore, it reduces the credibility of established media and other opinions and causes distortions in the process of forming political opinions. This causes social confusion and division, which harms civil society and its members and society as a whole. This is because it can cover facts and truths and spread false or distorted information to hinder the formation of sound public opinion. Naturally, such crackdowns and regulations on fake news are necessary. However, the biggest problem with fake news that we faced is that the current concept of fake news can be widespread and ambiguous. This is why the priority should be to find out what fake news is if legislation is to save damage to fake news. In a world of millions of news, they rely on manual human detection, so their scope is so limited that if the fake news is posted and deleted every minute, they cannot be manually responsible or executed. It can be a solution through the development of a system that provides reliable, automated exponential scores, namely trained machine learning.  solution through the development of a system that provides reliable, automated exponential scores, namely trained machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n",
    "\n",
    "##### DATASET 1:\n",
    "Dataset used\n",
    "The data source used for this project is LIAR dataset which contains 3 files with .tsv format for test, train and validation. Below is some description about the data files used for this project.\n",
    "\n",
    "LIAR: A BENCHMARK DATASET FOR FAKE NEWS DETECTION\n",
    "\n",
    "William Yang Wang, \"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection, to appear in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), short paper, Vancouver, BC, Canada, July 30-August 4, ACL.\n",
    "\n",
    "original dataset contained 13 variables/columns for train, test and validation sets as follows:\n",
    "\n",
    "*`Column 1`: the ID of the statement ([ID].json)\n",
    "\n",
    "*`Column 2`: the label \n",
    "\n",
    "(Label class contains: True, Mostly-true, Half-true, Barely-true, FALSE, Pants-fire)\n",
    "\n",
    "*`Column 3`: the statement\n",
    "\n",
    "*`Column 4`: the subject(s)\n",
    "\n",
    "*`Column 5`: the speaker\n",
    "\n",
    "*`Column 6`: the speaker's job title\n",
    "\n",
    "*`Column 7`: the state info\n",
    "\n",
    "*`Column 8`: the party affiliation\n",
    "\n",
    ": For Column 3 to 8, they are all discrete variables, so we didn’t consider to train this variables\n",
    "Column 9-13: the total credit history count, including the current statement.\n",
    "\n",
    "*`Column 9`: barely true counts\n",
    "\n",
    "*`Column 10`: false counts\n",
    "\n",
    "*`Column 11`: half true counts\n",
    "\n",
    "*`Column 12`: mostly true counts\n",
    "\n",
    "*`Column 13`: pants on fire counts\n",
    "\n",
    "*`Column 14`: the context (venue / location of the speech or statement)\n",
    "\n",
    "To make things simple we have chosen only 2 variables from this original dataset for this classification. The other variables can be added later to add some more complexity and enhance the features.\n",
    "\n",
    "Below are the columns used to create 3 datasets that have been in used in this project\n",
    "\n",
    "*`Column 1`: Statement (News headline or text)\n",
    "\n",
    "*`Column 2`: Label (Label class contains: True, False)\n",
    "\n",
    "You will see that newly created dataset has only 2 classes as compared to 6 from original classes. Below is method used for reducing the number of classes.\n",
    "\n",
    "*Original -- \tNew\n",
    "\n",
    "*True -- \t\tTrue\n",
    "\n",
    "*Mostly-true --  True\n",
    "\n",
    "*Half-true -- \t True\n",
    "\n",
    "*Barely-true --   False\n",
    "\n",
    "*False --\t\tFalse\n",
    "\n",
    "*Pants-fire --\tFalse\n",
    "\n",
    "The dataset used for this project were in csv format named train.csv, test.csv and valid.csv and can be found in repo. The original datasets are in \"liar\" folder in tsv format.\n",
    "\n",
    "\n",
    "\n",
    "##### DATASET 2:\n",
    "*politifact_fake.csv - Samples related to fake news collected from PolitiFact\n",
    "\n",
    "*politifact_real.csv - Samples related to real news collected from PolitiFact\n",
    "\n",
    "*gossipcop_fake.csv - Samples related to fake news collected from GossipCop\n",
    "\n",
    "*gossipcop_real.csv - Samples related to real news collected from GossipCop\n",
    "\n",
    "Each of the above CSV files is comma separated file and have the following columns\n",
    "\n",
    "*`id` - Unique identifider for each news\n",
    "\n",
    "*`url` - Url of the article from web that published that news\n",
    "\n",
    "*`title` - Title of the news article\n",
    "\n",
    "*`tweet_ids` - Tweet ids of tweets sharing the news (This field is list of tweet ids separated by tab)\n",
    "\n",
    "\n",
    "LIAR dataset and the politifact and gossipcop datasets all contain information related to news and its veracity. They can be combined to create a larger dataset that can be used for training models to detect fake news. Use one dataset to validate the other: two datasets contain different types of information, they can be used to validate each other: LIAR dataset can be used to validate the performance of models trained on the politifact and gossipcop datasets, and vice versa. We can use one dataset for feature engineering:The politifact and gossipcop datasets can be used to extract features related to the sources of news articles, which can be used to train models on the LIAR dataset. Also, this shows using ensemble methods: Ensemble methods combine the predictions of multiple models to improve accuracy. By training models on both datasets and using ensemble methods, the overall performance of the model can be improved. In summary, combining datasets, using one dataset for feature engineering, and using ensemble methods can all be effective ways to use our two datasets. The most effective approach will depend on the specific research question and the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Benchmark model: BERT (Bidirectional Encoder Representations from Transformers), FakeNewsNet: https://arxiv.org/pdf/1809.01286.pdf\n",
    "\n",
    "Solution: multivariable logistic regression\n",
    "\n",
    "Library used: numpy, pandas, scikit-learn\n",
    "\n",
    "Since our problem is a multiclass classification of evaluating if the news is True, Mostly-true, Half-true, Barely-true, FALSE, Pants-fire (6 labels), we will be using multi-class logistic regression. In order to process the test into input data, we will be using vectorization. Then we will be adding this vectorized input to variables from multiple columns and evaluate whether the news is true or false. Thus, we will be using multivariable logistic regression. First of all, we will be dividing the dataset into training/validation set and test set. Next, because vectorizing the text into word units creates too many variables, we will be using normalization. Because we will be needing strong normalization, we believe that using L1 normalization will be effective. The search plane itself is a vectorized text, therefore it is possible for it to become very high dimension which may result in being stuck in the local minima. In order to prevent this problem, we will be using momentum in our training. Also, in order to quickly do weight updates in training, we will be using mini-batch. In the validation step, we will be using K-fold cross validation to retreive the most accurate model. \n",
    "\n",
    "###### Text Processing\n",
    "After converting articles into token lists through CountVectorizer, TfidTransformer, and TfidVectorizer, vectorize them based on the count and frequency of appearance of tokens. In addition, TfidVectorier creates a BOW encoding vector that weights words in a TF-IDF, enabling document preprocessing. Using the TF-IDF method, you can manually adjust the parameter C to use an estimate that is a LogisticRegressionCV. Specifying cv=5, using kfold cross-validation for hyperparameter tuning. The criterion for model measurement is the accuracy of classification, and by setting it to n_jobs=-1, all CPU cores are dedicated. We’ll evaluate the performance by maximizing the number of iterations of the optimization algorithm.\n",
    "\n",
    "###### Modeling\n",
    "As a classification model, machine learning algorithms can be used \n",
    "a) Logistic Registration, b) SVM, c) Random Forest, and boosted trees(AdaBoost)\n",
    "\n",
    "First, modeling using the logistic regression algorithm.\n",
    "For modeling, separate train, validation sets and test set.\n",
    "Using the same dataset and the same text method and train_test_split, we can create logistic regression models that classify news as real or fake news. After organizing and preprocessing text data, performing feature extraction, and building and distributing a logistic regression classifier using the scikit-learn, evaluate the accuracy of the model.\n",
    "Second, Using SVM(Support Vector Machines) to build classification prediction models and compare prediction accuracy on datasets for verification.\n",
    "Third, tree-based random forest. Using Adaboost to fit the shallow decision tree on the train set, and additional classifiers can be placed on the same data to correct errors and can be weighted. \n",
    "Also, k-fold cross-validation, which is known to be useful because the amount of data is not enough to experiment with a larger amount of data on a smaller dataset to increase the statistical reliability of methodological performance measurements.\n",
    "\n",
    "###### Evaluation\n",
    "Confusion matrix : f1_score, report / Precision, recall \n",
    "\n",
    "\n",
    "###### What we see moving forward and limitations\n",
    "Using RNN, CNN, and LSTM, which are algorithms of deep learning could have analyzed with better performance. However, we need to review whether there are enough samples for deep learning, and we can later create a model using appropriate algorithms by devising how to classify new articles that we have never seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The evaluation metrics we will be using will be True positive rate (recall) and Accuracy. For recall, we will be using (correctly evaluating that news is false) / (correctly evaluating that news is false + evaluating that news is true when it is actually false). This recall rate is important for our problem because it tells us the rate of us being able to correctly evaluate that the news is false. For Accuracy, we will be using (correctly evaluating that news is false + evaluating that news is false when it is actually true) / (correctly evaluating that news is false + correctly evaluating that news is true + evaluating that news is true when it is actually false + evaluating that news is false when it is actually true). Accuracy is important for our problem because it tells us if our model was able to accurately make correct predictions in evaluating that the news is false. We will then plot an ROC curve to show the performance of the classification model and then find the AUC to provide an aggregate measure of performance across all possible classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"holannote\"></a>1.[^](#holan): holan, Angie D. The Media's Definition of Fake News vs. Donald Trump's. https://scholarship.law.unc.edu/cgi/viewcontent.cgi?article=1244&context=falr.<br>\n",
    "<a name=\"westnote\"></a>2.[^](#west): west, Darrell M. “How to Combat Fake News and Disinformation.” Brookings, Brookings, 9 Mar. 2022, https://www.brookings.edu/research/how-to-combat-fake-news-and-disinformation/.<br>\n",
    "<a name=\"arxivnote\"></a>3.[^](#arxive): Detecting Fake News using Machine Learning: A Systematic Literature Review https://arxiv.org/pdf/2102.04458.pdf<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
